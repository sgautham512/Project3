# -*- coding: utf-8 -*-
"""Project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SOSrs8ZdZ1SdCdqL5WrHh7g8elH4x6Ub
"""

import logging
!pip install boto3
from botocore.exceptions import ClientError

import os

# Replace with your actual AWS credentials
os.environ["AWS_ACCESS_KEY_ID"] = "XXXXXXXXXXXXXXXX"
os.environ["AWS_SECRET_ACCESS_KEY"] = "XXXXXXXXXXXXX"

import boto3
# Verify connection by fetching caller identity
sts_client = boto3.client('sts')
identity = sts_client.get_caller_identity()
print("Connected to AWS as:", identity["Arn"])

import boto3

# Let's use Amazon S3
s3 = boto3.resource('s3')

# Print out bucket names
for bucket in s3.buckets.all():
    print(bucket.name)

bucket_name = "my-new-etl-test-bucket-1"

s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'eu-central-1'})
print(f"Bucket {bucket_name} created successfully!")

with open('source1.csv', 'rb') as data:
    s3.Bucket(bucket_name).put_object(Key='source1.csv', Body=data)

with open('source1.json', 'rb') as data:
    s3.Bucket(bucket_name).put_object(Key='source1.json', Body=data)
with open('source1.xml', 'rb') as data:
    s3.Bucket(bucket_name).put_object(Key='source1.xml', Body=data)

s3 = boto3.client('s3')
s3.download_file(bucket_name, 'source1.xml', 'source1.xml')
s3.download_file(bucket_name, 'source1.json', 'source1.json')
s3.download_file(bucket_name, 'source1.csv', 'source1.csv')

import glob
import pandas as pd
import xml.etree.ElementTree
import datetime
import json
import logging
import os

# Setup logging configuration - this cofiguration not working for some reason
logging.basicConfig(
    filename='operations.txt',   # Name of the log file
    level=logging.INFO,          # Log level (INFO, DEBUG, etc.)
    format='%(asctime)s - %(message)s', # Log format with timestamp
    datefmt='%Y-%m-%d %H:%M:%S'  # Date and time format
)

# Create a logger object
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)         # Set the logging level

# Create a file handler
file_handler = logging.FileHandler('operations.txt')  # Name of the log file
file_handler.setLevel(logging.INFO)

# Create a formatter and add it to the handler
formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
file_handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(file_handler)

def ExtractDataFromCSV(csv_files):
  data_frame = pd.DataFrame()
  for file in csv_files:
    # Load the CSV into a Pandas DataFrame
    # data_frame = pd.read_csv(file)
    # print(data_frame)
    logger.info(f"contents of the {file} is appended to list")
    data_frame = pd.concat([data_frame,  pd.read_csv(file)], ignore_index=True)
  print(data_frame)
  return data_frame

def ExtractDataFromJSON(json_files):
  data_frame = pd.DataFrame()
  for file in json_files:
    # Load the json into a Pandas DataFrame
    # json_data = pd.read_json(file, lines=True)
    # print(json_data)
    logger.info(f"contents of the {file} is appended to list")
    data_frame = pd.concat([data_frame,  pd.read_json(file,lines=True)], ignore_index=True)
  print(data_frame)
  return data_frame

def ExtractDataFromXML(xml_files):
  data_frame = pd.DataFrame()
  for file in xml_files:
    # Load the json into a Pandas DataFrame
    # xml_data = pd.read_xml(file)
    # print(xml_data)
    logger.info(f"contents of the {file} is appended to list")
    data_frame = pd.concat([data_frame,  pd.read_xml(file)], ignore_index=True)
  print(data_frame)
  return data_frame

def ExtractData(file_path):
  main_data_frame = pd.DataFrame()
  try:
    # csv files are flat structure where things are represented in rows and columns
    all_csv_files = glob.glob(f"{file_path}/*.csv")
    print(all_csv_files)
    logger.info("Checks for all csv files in the directory")
    logger.info(f"list of csv files {all_csv_files}")
    main_data_frame = ExtractDataFromCSV(all_csv_files)
    print(f"main function print {main_data_frame}")

    # json structure is heirarchial and not always flat, might require special handling in some cases
    # default structure for json [....] bur one can also use the lines are delimiter
    # to read such files lines=TRUE should be set
    all_json_files = glob.glob(f"{file_path}/*.json")
    print(all_json_files)
    logger.info("Checks for all json files in the directory")
    logger.info(f"list of json files {all_json_files}")
    json_data_frame = ExtractDataFromJSON(all_json_files)
    main_data_frame = pd.concat([main_data_frame, json_data_frame], ignore_index=True)

    # straight forward method
    all_xml_files = glob.glob(f"{file_path}/*.xml")
    print(all_xml_files)
    logger.info("Checks for all xml files in the directory")
    logger.info(f"list of xml files {all_xml_files}")
    xml_data_frame = ExtractDataFromXML(all_xml_files)
    main_data_frame = pd.concat([main_data_frame, xml_data_frame], ignore_index=True)

     # remove the duplicate entries if there exists something
    main_data_frame = main_data_frame.drop_duplicates()
    main_data_frame = main_data_frame.reset_index(drop=True)
    logger.info(f"Duplicate entries are removed")


  except Exception as e:
    logging.info(f"Error occurred: {e}")

  return main_data_frame


current_directory = os.getcwd()
print(f"current directory - {current_directory}")
df = ExtractData(current_directory)
print(df)
#convert the entire height column from inches to meters
df["height"] = df["height"] * 0.0254
logger.info(f"heights are converted from inches to meter")

#convert the entire weight column from pounds to kgs
df["weight"] = df["weight"] * 0.453592
logger.info(f"weights are converted from pounds to kgs")

print(df)

df.to_csv('transformed_data.csv', index=False)
logger.info(f"A new csv file created with content from various appended and converted")

import time
unique_bucket_name = f"transformBucket-{int(time.time())}"

try:
    s3.create_bucket(Bucket=unique_bucket_name)
    print(f"Bucket '{unique_bucket_name}' created successfully.")
except Exception as e:
    print(f"Error: {e}")

import boto3

# Let's use Amazon S3
s3 = boto3.resource('s3')

with open('transformed_data.csv', 'rb') as data:
    s3.Bucket(bucket_name).put_object(Key='transformed_data.csv', Body=data)

!pip install mysql-connector-python

import mysql.connector
from mysql.connector import Error

try:
    connection = mysql.connector.connect(
    host = "database-2.cn48wookeoh0.eu-central-1.rds.amazonaws.com",
    port = 3306,
    user = "admin",
    password = "Sundar_90",
    )
    if connection.is_connected():
        print("Connection successful!")
        # Ping the server
        connection.ping(reconnect=True, attempts=3, delay=5)
        print("Ping successful!")
        mycursor = connection.cursor()
except Error as e:
    print(f"Error: {e}")

mycursor.execute("show databases")

for i in mycursor:
  print(i)

mycursor.execute("create database ETL")

def create_table_and_insert(csv_file_path, table_name, host, user, password, database):
    try:
        # Load the CSV into a Pandas DataFrame
        df = pd.read_csv(csv_file_path)

        # Connect to MySQL
        connection = mysql.connector.connect(
            host=host,
            user=user,
            password=password,
            database=database
        )
        cursor = connection.cursor()

        # Generate SQL table creation statement based on DataFrame
        columns = []
        for col in df.columns:
            # Infer data type
            dtype = df[col].dtype
            if pd.api.types.is_integer_dtype(dtype):
                sql_type = "INT"
            elif pd.api.types.is_float_dtype(dtype):
              sql_type = "FLOAT"
            elif pd.api.types.is_datetime64_any_dtype(dtype):
                sql_type = "DATETIME"
            else:
                max_length = max(df[col].astype(str).apply(len).max(), 255)
                sql_type = f"VARCHAR({max_length})"
            columns.append(f"`{col}` {sql_type}")

        create_table_query = f"CREATE TABLE IF NOT EXISTS `{table_name}` ({', '.join(columns)});"
        cursor.execute(create_table_query)
        print(f"Table `{table_name}` created successfully!")

        # Insert data into the table
        placeholders = ', '.join(['%s'] * len(df.columns))
        insert_query = f"INSERT INTO `{table_name}` VALUES ({placeholders})"
        cursor.executemany(insert_query, df.values.tolist())
        connection.commit()
        print(f"Inserted {cursor.rowcount} rows into `{table_name}`!")

    except Error as e:
        print(f"Error: {e}")
    finally:
        if 'connection' in locals() and connection.is_connected():
            cursor.close()
            connection.close()
            print("MySQL connection closed.")

# Example Usage
create_table_and_insert(
    csv_file_path="transformed_data.csv",       # Path to your CSV file
    table_name="transformed_data_table",  # Table name to create
    host = "database-2.cn48wookeoh0.eu-central-1.rds.amazonaws.com",
    user = "admin",
    password = "Sundar_90",
    database = "ETL"
 )

# mycursor.execute("show tables")
# for x in mycursor:
#   print(x)
# mycursor.execute("describe transformed_data_table")
# print(list(mycursor))

mycursor.execute("select* from ETL.transformed_data_table")
out = mycursor.fetchall()
from tabulate import tabulate
print(tabulate(out, headers=[i[0] for i in mycursor.description],showindex="always",tablefmt='psql'))